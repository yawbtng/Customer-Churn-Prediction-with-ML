{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "089bbcf5",
   "metadata": {},
   "source": [
    "# Customer Churn Prediction with Machine Learning\n",
    "\n",
    "**Course**: CS 5393 - Introduction to Machine Learning  \n",
    "**Student**: Yaw Boateng \n",
    "**Date**: October 11th, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project aims to develop a comprehensive machine learning pipeline for predicting customer churn in the banking sector. Using a dataset of 10,000 bank customers, we will build and compare multiple classification models to predict customer churn, extract probability scores for risk ranking and prioritization, evaluate models using both classification and regression metrics, deploy the best-performing model through a web application for real-time predictions, and integrate Large Language Models (LLMs) to explain predictions and generate personalized retention emails for customers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6637279",
   "metadata": {},
   "source": [
    "## Dataset Information\n",
    "\n",
    "### Source and Description\n",
    "- **Dataset**: Bank Customer Churn Dataset\n",
    "- **Source**: Kaggle\n",
    "- **URL**: https://www.kaggle.com/datasets/mathchi/churn-for-bank-customers\n",
    "- **Size**: 10,000 rows (excluding header)\n",
    "- **Features**: 14 columns total\n",
    "\n",
    "### Dataset Structure\n",
    "The dataset contains the following features:\n",
    "\n",
    "**Independent Variables (13 features)**:\n",
    "- `RowNumber`: Sequential row identifier\n",
    "- `CustomerId`: Unique customer identifier\n",
    "- `Surname`: Customer's last name\n",
    "- `CreditScore`: Customer's credit score (300-850)\n",
    "- `Geography`: Country of residence (France, Germany, Spain)\n",
    "- `Gender`: Customer gender (Male, Female)\n",
    "- `Age`: Customer age\n",
    "- `Tenure`: Number of years as bank customer\n",
    "- `Balance`: Account balance\n",
    "- `NumOfProducts`: Number of bank products used\n",
    "- `HasCrCard`: Credit card ownership (0/1)\n",
    "- `IsActiveMember`: Active membership status (0/1)\n",
    "- `EstimatedSalary`: Estimated annual salary\n",
    "\n",
    "**Target Variable (1 feature)**:\n",
    "- `Exited`: Churn indicator (0 = stayed, 1 = churned)\n",
    "\n",
    "### Data Types and Characteristics\n",
    "- **Numerical Features**: CreditScore, Age, Tenure, Balance, NumOfProducts, EstimatedSalary\n",
    "- **Categorical Features**: Geography, Gender, HasCrCard, IsActiveMember\n",
    "- **Binary Features**: HasCrCard, IsActiveMember, Exited\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ba448",
   "metadata": {},
   "source": [
    "## Machine Learning Task Definition\n",
    "\n",
    "### Dual Approach: Classification with Probability-Based Analysis\n",
    "\n",
    "This project employs a **hybrid approach** that combines binary classification with probability-based regression analysis to maximize both predictive accuracy and business utility.\n",
    "\n",
    "### Primary Task: Binary Classification\n",
    "\n",
    "**Objective**: Predict customer churn as a binary classification problem (churned vs. stayed).\n",
    "\n",
    "**Problem Formulation**:\n",
    "- **Input**: Customer features (CreditScore, Geography, Gender, Age, Tenure, Balance, etc.)\n",
    "- **Output**: Binary churn prediction (0 = stayed, 1 = churned)\n",
    "- **Goal**: Maximize recall for churners (class 1) to catch as many at-risk customers as possible\n",
    "\n",
    "**Justification for Classification Approach**:\n",
    "1. **Direct Business Decision**: Classification provides clear, actionable predictions (churn or no churn) that directly inform retention strategies\n",
    "2. **Class Imbalance Handling**: The dataset exhibits class imbalance, allowing us to explore techniques like SMOTE to improve model performance on the minority class\n",
    "3. **Business Priority**: For banking applications, recall (catching churners) is more critical than precision, as missing a churner has significant financial impact\n",
    "4. **Model Interpretability**: Classification models provide clear feature importance rankings and can generate probability scores for risk stratification\n",
    "5. **Standard Practice**: Binary classification is the standard approach for churn prediction in industry, making our solution directly applicable\n",
    "\n",
    "### Secondary Analysis: Probability-Based Evaluation\n",
    "\n",
    "**Objective**: Extract probability scores from classification models and evaluate them using regression-style metrics.\n",
    "\n",
    "**Approach**:\n",
    "- Use `.predict_proba()` to extract churn probability scores (0.0 to 1.0) from classification models\n",
    "- Calculate regression metrics (RMSE, MAE, R²) on probability predictions\n",
    "- Enable risk ranking and prioritization of customers by churn probability\n",
    "- Compare binary predictions vs probability-based risk assessment\n",
    "\n",
    "**Why Probability-Based Analysis?**:\n",
    "1. **Risk Stratification**: Probability scores enable banks to rank customers by churn risk and allocate resources efficiently\n",
    "2. **Threshold Optimization**: Banks can set different intervention thresholds (e.g., >0.7 = high priority) based on business needs\n",
    "3. **Resource Allocation**: Probability-based ranking helps prioritize retention efforts on highest-risk customers\n",
    "4. **Comprehensive Evaluation**: Regression metrics on probabilities provide additional insights into model calibration and probability estimation quality\n",
    "5. **Business Flexibility**: Allows banks to adjust retention strategies based on risk tiers (high/medium/low probability)\n",
    "\n",
    "### Success Metrics\n",
    "\n",
    "**Classification Metrics (Primary)**:\n",
    "- **Recall (Class 1)**: Primary metric - ability to catch customers who will actually churn\n",
    "- **Precision**: Of predicted churners, how many actually churned\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **Accuracy**: Overall correctness of predictions\n",
    "- **Confusion Matrix**: Detailed breakdown of predictions vs. actuals\n",
    "\n",
    "**Probability-Based Metrics (Secondary)**:\n",
    "- **RMSE (Root Mean Squared Error)**: Measures how well probability predictions match actual outcomes\n",
    "- **MAE (Mean Absolute Error)**: Average difference between predicted and actual probabilities\n",
    "- **R² Score**: Proportion of variance in churn explained by probability predictions\n",
    "- **Probability Calibration**: How well-calibrated the probability estimates are\n",
    "\n",
    "**Business Metrics**:\n",
    "- Maximize identification of at-risk customers for proactive retention efforts\n",
    "- Enable risk-based customer ranking and prioritization\n",
    "- Support tiered intervention strategies based on churn probability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dd89b",
   "metadata": {},
   "source": [
    "## Project Motivation\n",
    "\n",
    "### Learning Objectives\n",
    "This project serves multiple educational and practical purposes:\n",
    "\n",
    "1. **Practical ML Application**: Apply machine learning techniques to a real-world business problem that affects millions of customers globally\n",
    "2. **End-to-End Pipeline Development**: Learn to build complete ML workflows from data preprocessing to model deployment\n",
    "3. **Algorithm Comparison**: Gain hands-on experience comparing different machine learning algorithms and understanding their strengths/weaknesses\n",
    "4. **Business Analytics Integration**: Understand how ML models can drive business decisions and create value\n",
    "\n",
    "### Business Relevance\n",
    "Customer churn prediction is critical for the banking industry:\n",
    "\n",
    "- **Financial Impact**: Customer acquisition costs are 5-25x higher than retention costs\n",
    "- **Revenue Protection**: Preventing churn directly protects revenue streams\n",
    "- **Competitive Advantage**: Proactive retention strategies improve customer satisfaction and loyalty\n",
    "- **Resource Optimization**: Targeted interventions are more cost-effective than blanket retention programs\n",
    "\n",
    "### Technical Interest\n",
    "This project offers rich learning opportunities:\n",
    "\n",
    "- **Data Preprocessing**: Handle mixed data types (numerical, categorical, binary)\n",
    "- **Feature Engineering**: Create meaningful features from customer behavior data\n",
    "- **Model Selection**: Compare linear, tree-based, and ensemble methods\n",
    "- **Hyperparameter Optimization**: Learn systematic approaches to model tuning\n",
    "- **Deployment**: Build a web application for real-time predictions\n",
    "\n",
    "### Real-World Impact\n",
    "The project addresses a genuine business need:\n",
    "\n",
    "- **Banking Sector**: Help banks identify at-risk customers before they churn\n",
    "- **Customer Experience**: Enable personalized retention strategies\n",
    "- **Operational Efficiency**: Optimize customer service resource allocation\n",
    "- **Strategic Planning**: Provide data-driven insights for customer lifecycle management\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939623a0",
   "metadata": {},
   "source": [
    "## Proposed Approach\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "\n",
    "**Data Cleaning**:\n",
    "- Handle missing values using appropriate imputation strategies\n",
    "- Detect and treat outliers using IQR method or domain knowledge\n",
    "- Remove irrelevant features (RowNumber, CustomerId, Surname)\n",
    "\n",
    "**Feature Engineering**:\n",
    "- One-hot encode categorical variables (Geography, Gender)\n",
    "- Create interaction features (Age × Balance, Tenure × NumOfProducts)\n",
    "- Normalize numerical features using StandardScaler\n",
    "- Handle class imbalance using SMOTE (Synthetic Minority Oversampling Technique)\n",
    "\n",
    "**Data Splitting**:\n",
    "- Train/Test split: 80%/20%\n",
    "- Stratified sampling to maintain churn distribution\n",
    "- Random state (42) for reproducibility\n",
    "- Cross-validation for robust model evaluation\n",
    "\n",
    "### 2. Model Selection (7 Models)\n",
    "\n",
    "We will implement and compare the following classification models:\n",
    "\n",
    "1. **Logistic Regression**\n",
    "   - Baseline model for comparison\n",
    "   - Fast training and interpretable coefficients\n",
    "   - Provides probability scores for churn prediction\n",
    "   - Good starting point for binary classification problems\n",
    "\n",
    "2. **XGBoost Classifier**\n",
    "   - Gradient boosting ensemble method\n",
    "   - Often achieves state-of-the-art performance\n",
    "   - Built-in regularization and feature importance\n",
    "   - Handles non-linear relationships effectively\n",
    "\n",
    "3. **Decision Tree Classifier**\n",
    "   - Simple, interpretable tree-based model\n",
    "   - Can capture non-linear patterns\n",
    "   - Provides clear decision rules\n",
    "   - Prone to overfitting without regularization\n",
    "\n",
    "4. **Random Forest Classifier**\n",
    "   - Ensemble method combining multiple decision trees\n",
    "   - Reduces overfitting through averaging\n",
    "   - Handles non-linear relationships and feature interactions\n",
    "   - Provides feature importance rankings\n",
    "\n",
    "5. **Gaussian Naive Bayes**\n",
    "   - Probabilistic classifier based on Bayes' theorem\n",
    "   - Fast and works well with small datasets\n",
    "   - Assumes feature independence\n",
    "   - Good baseline for comparison\n",
    "\n",
    "6. **K-Nearest Neighbors (KNN) Classifier**\n",
    "   - Instance-based learning algorithm\n",
    "   - Classifies based on similarity to training examples\n",
    "   - Non-parametric method\n",
    "   - Sensitive to feature scaling (which we address with StandardScaler)\n",
    "\n",
    "7. **Support Vector Machine (SVM) Classifier**\n",
    "   - Finds optimal decision boundary\n",
    "   - Effective with high-dimensional data\n",
    "   - Can use different kernel functions for non-linear patterns\n",
    "   - Robust to outliers\n",
    "\n",
    "### 3. Hyperparameter Tuning\n",
    "\n",
    "**Optimization Strategy**:\n",
    "- Use GridSearchCV for smaller parameter spaces\n",
    "- Use RandomizedSearchCV for larger parameter spaces\n",
    "- 5-fold cross-validation for robust evaluation\n",
    "- Optimize for recall (class 1) as primary metric, given the business priority of catching churners\n",
    "\n",
    "**Key Hyperparameters**:\n",
    "- **Logistic Regression**: C (regularization strength), penalty (L1/L2)\n",
    "- **KNN**: n_neighbors, weights, distance metric\n",
    "- **SVM**: C, gamma, kernel type\n",
    "- **Random Forest**: n_estimators, max_depth, min_samples_split, class_weight\n",
    "- **XGBoost**: learning_rate, max_depth, n_estimators, subsample, scale_pos_weight (for class imbalance)\n",
    "- **Decision Tree**: max_depth, min_samples_split, min_samples_leaf, class_weight\n",
    "\n",
    "### 4. Model Evaluation\n",
    "\n",
    "**Classification Performance Metrics**:\n",
    "- **Recall (Class 1)**: Primary metric - ability to catch customers who will churn\n",
    "- **Precision**: Of predicted churners, how many actually churned\n",
    "- **F1-Score**: Harmonic mean of precision and recall (balanced metric)\n",
    "- **Accuracy**: Overall correctness of predictions\n",
    "- **Confusion Matrix**: Detailed breakdown of predictions vs. actuals\n",
    "- **Cross-validation**: 5-fold CV for robust performance estimates\n",
    "\n",
    "**Probability-Based Regression Metrics**:\n",
    "- **RMSE (Root Mean Squared Error)**: Evaluate how well probability predictions match actual churn outcomes\n",
    "- **MAE (Mean Absolute Error)**: Average absolute difference between predicted probabilities and actual outcomes\n",
    "- **R² Score**: Measure how well probability predictions explain variance in churn behavior\n",
    "- **Probability Distribution Analysis**: Visualize and analyze the distribution of churn probabilities\n",
    "- **Calibration Analysis**: Assess how well-calibrated probability estimates are (using calibration curves)\n",
    "\n",
    "**Probability Extraction and Analysis**:\n",
    "- Extract probability scores using `.predict_proba()` from all classification models\n",
    "- Rank customers by churn probability to enable risk-based prioritization\n",
    "- Create risk tiers (high/medium/low) based on probability thresholds\n",
    "- Compare probability distributions across different customer segments\n",
    "- Analyze probability calibration to ensure reliable risk estimates\n",
    "\n",
    "**Model Comparison**:\n",
    "- Compare recall scores across all models (primary criterion for classification)\n",
    "- Compare RMSE and MAE for probability predictions (evaluation of probability estimation quality)\n",
    "- Statistical significance testing between models\n",
    "- Learning curves to assess bias-variance tradeoff\n",
    "- Feature importance analysis for business insights (using XGBoost)\n",
    "- Confusion matrix analysis to understand prediction patterns\n",
    "- Probability calibration comparison across models\n",
    "\n",
    "### 5. Deployment Strategy\n",
    "\n",
    "**Web Application Development**:\n",
    "- Build interactive web interface using Streamlit\n",
    "- User-friendly input forms for customer data\n",
    "- Real-time churn probability predictions\n",
    "- Visualization of prediction confidence and feature contributions\n",
    "\n",
    "**Model Serving**:\n",
    "- Save trained models using joblib/pickle\n",
    "- Implement model versioning and A/B testing capabilities\n",
    "- Add input validation and error handling\n",
    "- Create API endpoints for integration with other systems\n",
    "\n",
    "### 6. Advanced Techniques\n",
    "\n",
    "**Class Imbalance Handling**:\n",
    "- Apply SMOTE to balance the training dataset\n",
    "- Generate synthetic samples of the minority class (churners)\n",
    "- Improve model's ability to learn patterns from churned customers\n",
    "\n",
    "**Ensembling**:\n",
    "- Implement voting classifiers to combine multiple models\n",
    "- Use hard voting and soft voting strategies\n",
    "- Leverage strengths of different algorithms for improved performance\n",
    "\n",
    "### 7. LLM Integration for Explainability and Personalization\n",
    "\n",
    "**Prediction Explanation**:\n",
    "- Use Groq LLM to explain model predictions in natural language\n",
    "- Explain feature contributions and their impact on churn probability\n",
    "- Generate human-readable interpretations of technical model outputs\n",
    "- Leverage Groq's fast inference for real-time explanations\n",
    "\n",
    "**Personalized Email Generation**:\n",
    "- Generate personalized retention emails based on churn probability predictions\n",
    "- Include specific recommendations based on customer features and risk factors\n",
    "- Create actionable retention strategies tailored to each customer\n",
    "- Utilize Groq's low-cost, high-speed inference for efficient email generation\n",
    "\n",
    "**Implementation Approach**:\n",
    "- Integrate Groq API (OpenAI-compatible, easy to use)\n",
    "- Design effective prompts for explanation and email generation\n",
    "- Extract feature importance and contributions from models\n",
    "- Format customer data and predictions for LLM processing\n",
    "- Display explanations and emails in web application\n",
    "- Take advantage of Groq's speed and affordability for responsive user experience\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c56c3a",
   "metadata": {},
   "source": [
    "## Expected Outcomes\n",
    "\n",
    "### Technical Deliverables\n",
    "\n",
    "1. **Trained Models**\n",
    "   - Seven optimized classification models with hyperparameter tuning\n",
    "   - Performance comparison across all models\n",
    "   - Best-performing model selected based on recall (class 1) and business criteria\n",
    "   - Models saved for deployment (XGBoost, Random Forest, Decision Tree, Naive Bayes, KNN, SVM, Logistic Regression)\n",
    "\n",
    "2. **Model Performance Analysis**\n",
    "   - **Classification Metrics**: Comprehensive evaluation (Recall, Precision, F1-Score, Accuracy)\n",
    "   - **Probability-Based Metrics**: RMSE, MAE, R² scores on probability predictions\n",
    "   - Cross-validation results for robust performance estimates\n",
    "   - Feature importance rankings and business insights (using XGBoost)\n",
    "   - Confusion matrix analysis for each model\n",
    "   - Probability calibration analysis and visualization\n",
    "   - Comparison of models with and without SMOTE\n",
    "   - Risk ranking and stratification based on probability scores\n",
    "\n",
    "3. **Data Insights**\n",
    "   - Key factors driving customer churn\n",
    "   - Customer segmentation based on churn risk (binary classification)\n",
    "   - **Probability-based risk ranking**: Customers ranked by churn probability for prioritization\n",
    "   - **Risk tier analysis**: High/medium/low risk customer categorization\n",
    "   - Recommendations for retention strategies\n",
    "   - Visualization of model performance and feature contributions\n",
    "   - Probability distribution analysis across customer segments\n",
    "\n",
    "4. **Web Application**\n",
    "   - Interactive interface for churn prediction\n",
    "   - Real-time model inference capabilities\n",
    "   - User-friendly input forms and result visualization\n",
    "   - Display churn prediction (churned/stayed) with probability scores\n",
    "   - **Probability-based risk ranking**: Sort and filter customers by churn probability\n",
    "   - **Risk tier visualization**: Display high/medium/low risk categories\n",
    "   - LLM-powered prediction explanations\n",
    "   - Personalized retention email generation\n",
    "   - API endpoints for integration with other systems\n",
    "\n",
    "5. **LLM Integration**\n",
    "   - Natural language explanations of model predictions\n",
    "   - Feature contribution explanations\n",
    "   - Personalized retention email generation\n",
    "   - Actionable recommendations for customer retention\n",
    "\n",
    "### Business Value\n",
    "\n",
    "1. **Customer Retention Strategy**\n",
    "   - Identify high-risk customers for proactive intervention\n",
    "   - **Probability-based prioritization**: Rank customers by churn probability to focus on highest-risk individuals\n",
    "   - **Tiered intervention strategies**: Different retention approaches for high/medium/low probability customers\n",
    "   - Optimize resource allocation for customer service based on risk levels\n",
    "   - Focus on maximizing recall to catch as many potential churners as possible\n",
    "   - Enable data-driven decision making through both binary predictions and probability scores\n",
    "\n",
    "2. **Risk Management**\n",
    "   - Early warning system for customer churn\n",
    "   - Data-driven approach to customer lifecycle management\n",
    "   - Improved customer satisfaction through targeted interventions\n",
    "\n",
    "3. **Operational Efficiency**\n",
    "   - Automated churn prediction reduces manual analysis\n",
    "   - Scalable solution for large customer bases\n",
    "   - Integration capabilities with existing banking systems\n",
    "   - LLM-generated explanations make predictions accessible to non-technical stakeholders\n",
    "   - Automated email generation reduces manual content creation\n",
    "\n",
    "### Learning Outcomes\n",
    "\n",
    "1. **Technical Skills**\n",
    "   - End-to-end machine learning pipeline development\n",
    "   - Model comparison and hyperparameter optimization\n",
    "   - Web application development and model deployment\n",
    "   - Data preprocessing and feature engineering\n",
    "\n",
    "2. **Business Understanding**\n",
    "   - Application of ML to real-world business problems\n",
    "   - Customer analytics and retention strategies\n",
    "   - Performance metrics interpretation for business decisions\n",
    "   - LLM integration for explainable AI and natural language generation\n",
    "\n",
    "3. **Project Management**\n",
    "   - Structured approach to ML project development\n",
    "   - Documentation and reproducibility best practices\n",
    "   - Model validation and deployment considerations\n",
    "\n",
    "4. **Advanced Skills**\n",
    "   - LLM API integration and prompt engineering\n",
    "   - Explainable AI techniques\n",
    "   - Natural language generation for business applications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f9c41",
   "metadata": {},
   "source": [
    "## Project Timeline and Milestones\n",
    "\n",
    "### Phase 1: Data Exploration and Preprocessing (Week 1)\n",
    "- [ ] Load and explore the dataset\n",
    "- [ ] Perform exploratory data analysis (EDA)\n",
    "- [ ] Handle missing values and outliers\n",
    "- [ ] Engineer features and encode categorical variables\n",
    "- [ ] Split data into train/validation/test sets\n",
    "\n",
    "### Phase 2: Model Development (Week 2)\n",
    "- [ ] Implement Logistic Regression baseline model\n",
    "- [ ] Implement XGBoost Classifier\n",
    "- [ ] Implement Decision Tree Classifier\n",
    "- [ ] Implement Random Forest Classifier\n",
    "- [ ] Implement Gaussian Naive Bayes\n",
    "- [ ] Implement K-Nearest Neighbors Classifier\n",
    "- [ ] Implement Support Vector Machine Classifier\n",
    "\n",
    "### Phase 3: Model Optimization (Week 3)\n",
    "- [ ] Perform hyperparameter tuning for all models (optimize for recall)\n",
    "- [ ] Cross-validation and performance evaluation\n",
    "- [ ] Model comparison and selection (focus on recall for class 1)\n",
    "- [ ] Feature importance analysis using XGBoost\n",
    "- [ ] Implement SMOTE for class imbalance handling\n",
    "- [ ] Compare models with and without SMOTE\n",
    "- [ ] Implement ensemble methods (voting classifiers)\n",
    "- [ ] Extract probability scores from all models using `.predict_proba()`\n",
    "- [ ] Calculate regression metrics (RMSE, MAE, R²) on probability predictions\n",
    "- [ ] Perform probability calibration analysis\n",
    "- [ ] Create risk ranking and tiering based on probability scores\n",
    "\n",
    "### Phase 4: Deployment and LLM Integration (Week 4)\n",
    "- [ ] Build web application for model inference\n",
    "- [ ] Integrate LLM API for prediction explanations\n",
    "- [ ] Implement personalized email generation\n",
    "- [ ] Create user interface with explanation and email features\n",
    "- [ ] Test and validate deployed model and LLM integration\n",
    "\n",
    "### Phase 5: Documentation and Finalization (Week 5)\n",
    "- [ ] Document results and create visualizations\n",
    "- [ ] Document both classification and probability-based analysis results\n",
    "- [ ] Create visualizations comparing binary predictions vs probability scores\n",
    "- [ ] Test LLM explanations and email quality\n",
    "- [ ] Prepare final presentation and report\n",
    "- [ ] Document LLM integration and prompt engineering approach\n",
    "- [ ] Document risk ranking methodology and business applications\n",
    "\n",
    "### Optional Challenges\n",
    "- [ ] **Challenge 1**: Retrain ML models with different feature engineering and preprocessing techniques to increase accuracy\n",
    "- [ ] **Challenge 2**: Experiment with additional ensemble techniques (StackingClassifier, etc.) and compare performance\n",
    "- [ ] **Challenge 3**: Experiment with different LLMs and prompting techniques to generate better explanations and emails\n",
    "- [ ] **Challenge 4**: Host ML models on cloud with API serving capabilities\n",
    "- [ ] **Challenge 5**: Train ML models on different datasets and achieve highest accuracy possible\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This project will provide comprehensive experience in applying machine learning to solve real-world business problems. By developing a complete pipeline from data preprocessing to model deployment, and integrating LLMs for explainability and personalization, we will gain valuable insights into customer churn prediction while building practical skills in machine learning, data science, web development, and LLM integration.\n",
    "\n",
    "The combination of technical rigor, business relevance, and AI explainability makes this project an excellent learning opportunity that bridges academic concepts with practical applications in the banking industry. Through our dual approach of binary classification and probability-based analysis, we will address the critical business need of identifying at-risk customers with both clear predictions and nuanced risk assessment. The focus on maximizing recall ensures we catch as many potential churners as possible, while probability scores enable sophisticated risk ranking and resource allocation. The integration of SMOTE for class imbalance handling, ensemble methods, and probability-based evaluation further demonstrates advanced ML techniques, while also exploring the intersection of traditional ML and modern LLM technologies. This comprehensive approach provides banks with both actionable binary decisions and flexible probability-based strategies for customer retention.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
